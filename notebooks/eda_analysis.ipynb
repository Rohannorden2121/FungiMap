{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b506b11",
   "metadata": {},
   "source": [
    "# FungiMap: Exploratory Data Analysis\n",
    "\n",
    "This notebook implements the exploratory data analysis phase of the FungiMap pipeline. The analysis includes the following steps:\n",
    "\n",
    "1. **Sample Collection and Validation**: Quality control and metadata completeness assessment\n",
    "2. **Read Quality Assessment**: FastQC analysis of raw sequencing data  \n",
    "3. **Taxonomic Profiling**: Kraken2-based fungal content estimation\n",
    "4. **Data Quality Filtering**: Sample selection based on fungal signal strength\n",
    "5. **Resource Planning**: Computational resource estimation for downstream analysis\n",
    "6. **Metadata Integration**: Sample annotation following community standards\n",
    "\n",
    "The analysis prioritizes samples with:\n",
    "- High metadata completeness (>70% of required fields)\n",
    "- Sufficient fungal genomic content (>10% classified reads)\n",
    "- Adequate sequencing depth (>1M read pairs)\n",
    "- Low host contamination (<50% host-derived reads)\n",
    "\n",
    "This exploratory phase generates validated sample manifests and resource allocation plans for the full FungiMap analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9469506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pysradb import SRAweb\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# Enable interactive plots in notebook\n",
    "try:\n",
    "    import IPython\n",
    "    IPython.get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set up project paths\n",
    "PROJECT_ROOT = Path('/Users/rohannorden/My Code/mycology-project')\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "CONFIG_DIR = PROJECT_ROOT / 'config'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [DATA_DIR, RESULTS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load configuration\n",
    "with open(CONFIG_DIR / 'eda_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Set up custom visualization functions\n",
    "def create_quality_heatmap(data, title):\n",
    "    \"\"\"Create an interactive quality heatmap.\"\"\"\n",
    "    fig = go.Figure(data=go.Heatmap(z=data))\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Position in Read\",\n",
    "        yaxis_title=\"Quality Score\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def plot_read_quality_distribution(data, sample_name):\n",
    "    \"\"\"Plot read quality distribution with confidence intervals.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.boxplot(data=data, ax=ax)\n",
    "    sns.swarmplot(data=data, color=\".25\", size=2, ax=ax)\n",
    "    ax.set_title(f\"Read Quality Distribution - {sample_name}\")\n",
    "    ax.set_xlabel(\"Position in Read\")\n",
    "    ax.set_ylabel(\"Quality Score\")\n",
    "    return fig\n",
    "\n",
    "def create_taxonomy_sunburst(taxa_data):\n",
    "    \"\"\"Create interactive sunburst plot for taxonomic data.\"\"\"\n",
    "    fig = px.sunburst(taxa_data, \n",
    "                      path=['kingdom', 'phylum', 'class', 'order'],\n",
    "                      values='abundance',\n",
    "                      title=\"Taxonomic Distribution\")\n",
    "    return fig\n",
    "\n",
    "print(\"Environment setup complete. Configuration loaded.\")\n",
    "print(\"Advanced visualization functions initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5de5fa",
   "metadata": {},
   "source": [
    "## Data Harvesting Functions\n",
    "\n",
    "The following cells implement functions to:\n",
    "1. Query SRA/ENA/MGnify databases\n",
    "2. Process and validate metadata\n",
    "3. Check MIxS/ENVO compliance\n",
    "4. Calculate data completeness scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHarvester:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize data harvester with configuration.\"\"\"\n",
    "        self.config = config\n",
    "        self.db = SRAweb()\n",
    "        self.mgnify_api = \"https://www.ebi.ac.uk/metagenomics/api/v1\"\n",
    "        self.manifest = []\n",
    "        self.candidates = []\n",
    "        \n",
    "    def query_sra(self):\n",
    "        \"\"\"Query SRA database for metagenome and metatranscriptome datasets.\"\"\"\n",
    "        query_terms = self.config['databases']['sra']['query_terms']\n",
    "        query = \" OR \".join([f'\"{term}\"[All Fields]' for term in query_terms])\n",
    "        query += ' AND (\"metagenome\"[Source] OR \"metatranscriptome\"[Source])'\n",
    "        \n",
    "        print(f\"Querying SRA with: {query}\")\n",
    "        results = self.db.search(query)\n",
    "        \n",
    "        # Log the query in manifest\n",
    "        self.manifest.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'database': 'SRA',\n",
    "            'query': query,\n",
    "            'results_count': len(results)\n",
    "        })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def query_mgnify(self):\n",
    "        \"\"\"Query MGnify for fungal metagenomes.\"\"\"\n",
    "        filters = self.config['databases']['mgnify']['filters']\n",
    "        params = {\n",
    "            'experiment_type': filters['experiment_type'],\n",
    "            'biome': ','.join(filters['biome'])\n",
    "        }\n",
    "        \n",
    "        response = requests.get(f\"{self.mgnify_api}/studies\", params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        # Log the query\n",
    "        self.manifest.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'database': 'MGnify',\n",
    "            'query_params': params,\n",
    "            'results_count': len(data.get('data', []))\n",
    "        })\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def check_mixs_compliance(self, metadata):\n",
    "        \"\"\"Check MIxS/ENVO metadata completeness.\"\"\"\n",
    "        required_fields = self.config['output_formats']['metadata']\n",
    "        present_fields = sum(1 for field in required_fields \n",
    "                           if field in metadata and metadata[field])\n",
    "        return (present_fields / len(required_fields)) * 100\n",
    "    \n",
    "    def process_candidate(self, record):\n",
    "        \"\"\"Process a single candidate dataset.\"\"\"\n",
    "        metadata = record._asdict()\n",
    "        \n",
    "        # Calculate metadata completeness\n",
    "        completeness = self.check_mixs_compliance(metadata)\n",
    "        \n",
    "        # Basic validation against thresholds\n",
    "        read_pairs = int(metadata.get('spots', 0))\n",
    "        meets_criteria = (\n",
    "            read_pairs >= self.config['data_selection']['min_raw_read_pairs'],\n",
    "            completeness >= self.config['data_selection']['min_metadata_completeness']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'accession': metadata.get('run_accession'),\n",
    "            'habitat': metadata.get('sample_attribute', '').split(';')[0],\n",
    "            'raw_read_pairs': read_pairs,\n",
    "            'estimated_size_gb': float(metadata.get('size_MB', 0)) / 1024,\n",
    "            'metadata_completeness': completeness,\n",
    "            'metadata_url': f\"https://www.ncbi.nlm.nih.gov/sra/{metadata.get('run_accession')}\",\n",
    "            'passes_criteria': all(meets_criteria)\n",
    "        }\n",
    "    \n",
    "    def harvest_candidates(self):\n",
    "        \"\"\"Main method to harvest and process candidate datasets.\"\"\"\n",
    "        # Query databases\n",
    "        sra_results = self.query_sra()\n",
    "        mgnify_results = self.query_mgnify()\n",
    "        \n",
    "        # Process candidates\n",
    "        print(\"Processing candidates...\")\n",
    "        for record in tqdm(sra_results.itertuples(), total=len(sra_results)):\n",
    "            candidate = self.process_candidate(record)\n",
    "            self.candidates.append(candidate)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(self.candidates)\n",
    "        \n",
    "        # Save manifest\n",
    "        manifest_df = pd.DataFrame(self.manifest)\n",
    "        manifest_df.to_csv(RESULTS_DIR / 'manifest_draft.csv', index=False)\n",
    "        \n",
    "        return df\n",
    "\n",
    "harvester = DataHarvester(config)\n",
    "print(\"Data harvester initialized and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bea679",
   "metadata": {},
   "source": [
    "## Fetch and Process Candidate Accessions\n",
    "\n",
    "Now we'll execute the data harvesting process and analyze the results. This will:\n",
    "1. Query both SRA and MGnify databases\n",
    "2. Process and validate the metadata\n",
    "3. Generate initial candidate list\n",
    "4. Filter based on our quality criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive scatter plot\n",
    "fig = px.scatter(\n",
    "    df_viz, \n",
    "    x='total_reads', \n",
    "    y='avg_length',\n",
    "    color='env_broad_scale',\n",
    "    size='metadata_completeness',\n",
    "    hover_data=['sample_id', 'geo_loc_name', 'host'],\n",
    "    title=\"FungiMap Dataset Overview\",\n",
    "    labels={\n",
    "        'total_reads': 'Total Reads',\n",
    "        'avg_length': 'Average Read Length (bp)',\n",
    "        'env_broad_scale': 'Environment Type'\n",
    "    },\n",
    "    width=900,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title_x=0.5,\n",
    "    legend=dict(\n",
    "        orientation=\"v\",\n",
    "        yanchor=\"top\",\n",
    "        y=1,\n",
    "        xanchor=\"left\",\n",
    "        x=1.01\n",
    "    ),\n",
    "    margin=dict(r=150)\n",
    ")\n",
    "\n",
    "# Add sample size reference\n",
    "fig.add_annotation(\n",
    "    text=\"Bubble size = Metadata Completeness (%)\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.02, y=0.98,\n",
    "    showarrow=False,\n",
    "    font=dict(size=10, color=\"gray\")\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f405b",
   "metadata": {},
   "source": [
    "## Sample Processing Pipeline\n",
    "\n",
    "Next, we'll implement the processing pipeline for our subsamples. This includes:\n",
    "1. Downloading small subsamples (100k-500k reads)\n",
    "2. Running QC checks\n",
    "3. Performing taxonomic profiling\n",
    "4. Calculating key statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41377ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleProcessor:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize sample processor with configuration.\"\"\"\n",
    "        self.config = config\n",
    "        self.subsample_dir = DATA_DIR / 'subsamples'\n",
    "        self.subsample_dir.mkdir(exist_ok=True)\n",
    "        self.results = []\n",
    "        self.qc_metrics = {}\n",
    "        \n",
    "    def fetch_subsample(self, accession):\n",
    "        \"\"\"Fetch a subsample of reads from SRA.\"\"\"\n",
    "        subsample_size = self.config['data_selection']['subsample_size']\n",
    "        output_file = self.subsample_dir / f\"{accession}_subsample.fastq\"\n",
    "        \n",
    "        # Use fastq-dump to get subsample\n",
    "        cmd = f\"fastq-dump --split-spot --maxspot {subsample_size} \"\n",
    "        cmd += f\"--outdir {self.subsample_dir} {accession}\"\n",
    "        \n",
    "        # Execute command and log\n",
    "        os.system(cmd)\n",
    "        return output_file\n",
    "    \n",
    "    def parse_fastqc_data(self, fastqc_data_file):\n",
    "        \"\"\"Parse FastQC data file and return structured metrics.\"\"\"\n",
    "        metrics = {}\n",
    "        current_section = None\n",
    "        data = []\n",
    "        \n",
    "        with open(fastqc_data_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>>'):\n",
    "                    if current_section and data:\n",
    "                        metrics[current_section] = pd.DataFrame(data)\n",
    "                        data = []\n",
    "                    current_section = line.split('\\t')[0][2:]\n",
    "                elif line.startswith('#'):\n",
    "                    continue\n",
    "                else:\n",
    "                    data.append(line.split('\\t'))\n",
    "                    \n",
    "        if current_section and data:\n",
    "            metrics[current_section] = pd.DataFrame(data)\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def run_qc(self, fastq_file):\n",
    "        \"\"\"Run FastQC on subsample with enhanced metrics.\"\"\"\n",
    "        output_dir = self.subsample_dir / 'fastqc'\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        cmd = f\"fastqc -o {output_dir} {fastq_file}\"\n",
    "        os.system(cmd)\n",
    "        \n",
    "        # Parse FastQC results\n",
    "        fastqc_data = self.parse_fastqc_data(\n",
    "            output_dir / f\"{fastq_file.stem}_fastqc/fastqc_data.txt\"\n",
    "        )\n",
    "        \n",
    "        # Calculate advanced QC metrics\n",
    "        qc_metrics = {\n",
    "            'basic_statistics': self._calculate_basic_stats(fastqc_data),\n",
    "            'sequence_quality': self._analyze_sequence_quality(fastqc_data),\n",
    "            'gc_content': self._analyze_gc_content(fastqc_data),\n",
    "            'sequence_length': self._analyze_sequence_length(fastqc_data),\n",
    "            'overrepresented': self._analyze_overrepresented(fastqc_data)\n",
    "        }\n",
    "        \n",
    "        # Store QC metrics for visualization\n",
    "        self.qc_metrics[fastq_file.stem] = qc_metrics\n",
    "        \n",
    "        return qc_metrics\n",
    "    \n",
    "    def _calculate_basic_stats(self, fastqc_data):\n",
    "        \"\"\"Calculate basic statistics from FastQC data.\"\"\"\n",
    "        if 'Basic Statistics' in fastqc_data:\n",
    "            stats_df = fastqc_data['Basic Statistics']\n",
    "            return dict(zip(stats_df[0], stats_df[1]))\n",
    "        return {}\n",
    "    \n",
    "    def _analyze_sequence_quality(self, fastqc_data):\n",
    "        \"\"\"Analyze per base sequence quality.\"\"\"\n",
    "        if 'Per base sequence quality' in fastqc_data:\n",
    "            qual_df = fastqc_data['Per base sequence quality']\n",
    "            return {\n",
    "                'mean_quality': float(qual_df[1].astype(float).mean()),\n",
    "                'min_quality': float(qual_df[1].astype(float).min()),\n",
    "                'quality_trend': qual_df[1].astype(float).tolist()\n",
    "            }\n",
    "        return {}\n",
    "    \n",
    "    def _analyze_gc_content(self, fastqc_data):\n",
    "        \"\"\"Analyze GC content distribution.\"\"\"\n",
    "        if 'Per sequence GC content' in fastqc_data:\n",
    "            gc_df = fastqc_data['Per sequence GC content']\n",
    "            gc_values = gc_df[0].astype(float)\n",
    "            gc_counts = gc_df[1].astype(float)\n",
    "            return {\n",
    "                'mean_gc': float(np.average(gc_values, weights=gc_counts)),\n",
    "                'gc_distribution': dict(zip(gc_values.tolist(), gc_counts.tolist()))\n",
    "            }\n",
    "        return {}\n",
    "    \n",
    "    def _analyze_sequence_length(self, fastqc_data):\n",
    "        \"\"\"Analyze sequence length distribution.\"\"\"\n",
    "        if 'Sequence Length Distribution' in fastqc_data:\n",
    "            len_df = fastqc_data['Sequence Length Distribution']\n",
    "            return {\n",
    "                'length_range': f\"{len_df[0].iloc[0]}-{len_df[0].iloc[-1]}\",\n",
    "                'distribution': dict(zip(len_df[0].tolist(), len_df[1].astype(float).tolist()))\n",
    "            }\n",
    "        return {}\n",
    "    \n",
    "    def _analyze_overrepresented(self, fastqc_data):\n",
    "        \"\"\"Analyze overrepresented sequences.\"\"\"\n",
    "        if 'Overrepresented sequences' in fastqc_data:\n",
    "            over_df = fastqc_data['Overrepresented sequences']\n",
    "            return {\n",
    "                'num_overrepresented': len(over_df),\n",
    "                'top_sequences': over_df.head(5).to_dict('records')\n",
    "            }\n",
    "        return {}\n",
    "    \n",
    "    def parse_kraken_report(self, report_file):\n",
    "        \"\"\"Parse Kraken2 report with enhanced taxonomic analysis.\"\"\"\n",
    "        taxa_data = []\n",
    "        \n",
    "        with open(report_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 6:\n",
    "                    percentage = float(parts[0])\n",
    "                    reads = int(parts[1])\n",
    "                    rank = parts[3]\n",
    "                    taxid = parts[4]\n",
    "                    name = parts[5].strip()\n",
    "                    \n",
    "                    taxa_data.append({\n",
    "                        'percentage': percentage,\n",
    "                        'reads': reads,\n",
    "                        'rank': rank,\n",
    "                        'taxid': taxid,\n",
    "                        'name': name\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(taxa_data)\n",
    "    \n",
    "    def run_kraken(self, fastq_file):\n",
    "        \"\"\"Run Kraken2 for taxonomic profiling with enhanced analysis.\"\"\"\n",
    "        output_prefix = self.subsample_dir / f\"{fastq_file.stem}_kraken\"\n",
    "        output_file = f\"{output_prefix}.txt\"\n",
    "        report_file = f\"{output_prefix}_report.txt\"\n",
    "        \n",
    "        cmd = f\"kraken2 --db {self.config['qc_parameters']['kraken2']['db']} \"\n",
    "        cmd += f\"--confidence {self.config['qc_parameters']['kraken2']['confidence']} \"\n",
    "        cmd += f\"--output {output_file} --report {report_file} {fastq_file}\"\n",
    "        \n",
    "        os.system(cmd)\n",
    "        \n",
    "        # Parse and analyze Kraken results\n",
    "        taxa_df = self.parse_kraken_report(report_file)\n",
    "        \n",
    "        # Calculate advanced taxonomic metrics\n",
    "        tax_metrics = {\n",
    "            'classified_reads_percent': float(taxa_df['percentage'].sum()),\n",
    "            'unique_taxa': len(taxa_df),\n",
    "            'diversity_index': self._calculate_diversity_index(taxa_df),\n",
    "            'top_taxa': taxa_df.nlargest(10, 'percentage')[['name', 'percentage', 'reads']].to_dict('records'),\n",
    "            'rank_distribution': taxa_df.groupby('rank')['percentage'].sum().to_dict()\n",
    "        }\n",
    "        \n",
    "        return tax_metrics\n",
    "    \n",
    "    def _calculate_diversity_index(self, taxa_df):\n",
    "        \"\"\"Calculate Shannon diversity index from taxonomic data.\"\"\"\n",
    "        proportions = taxa_df['percentage'] / 100\n",
    "        proportions = proportions[proportions > 0]  # Remove zeros\n",
    "        return float(-np.sum(proportions * np.log(proportions)))\n",
    "    \n",
    "    def process_sample(self, accession):\n",
    "        \"\"\"Process a single sample through the pipeline with enhanced analysis.\"\"\"\n",
    "        try:\n",
    "            # Fetch subsample\n",
    "            fastq_file = self.fetch_subsample(accession)\n",
    "            \n",
    "            # Run QC with enhanced metrics\n",
    "            qc_results = self.run_qc(fastq_file)\n",
    "            \n",
    "            # Run Kraken with enhanced taxonomic analysis\n",
    "            tax_results = self.run_kraken(fastq_file)\n",
    "            \n",
    "            # Compile comprehensive results\n",
    "            results = {\n",
    "                'accession': accession,\n",
    "                'qc_metrics': qc_results,\n",
    "                'taxonomic_profile': tax_results,\n",
    "                'quality_score': self._calculate_quality_score(qc_results, tax_results),\n",
    "                'processing_completed': True\n",
    "            }\n",
    "            \n",
    "            self.results.append(results)\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {accession}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _calculate_quality_score(self, qc_results, tax_results):\n",
    "        \"\"\"Calculate overall quality score based on multiple metrics.\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        # Sequence quality score (0-100)\n",
    "        if 'sequence_quality' in qc_results:\n",
    "            mean_qual = qc_results['sequence_quality']['mean_quality']\n",
    "            scores.append(min(100, mean_qual * 2.5))  # Scale quality to 0-100\n",
    "            \n",
    "        # GC content score (0-100)\n",
    "        if 'gc_content' in qc_results:\n",
    "            gc_mean = qc_results['gc_content']['mean_gc']\n",
    "            gc_score = 100 - abs(gc_mean - 50) * 2  # Penalize deviation from 50%\n",
    "            scores.append(max(0, gc_score))\n",
    "            \n",
    "        # Taxonomic classification score (0-100)\n",
    "        classified_pct = tax_results['classified_reads_percent']\n",
    "        scores.append(min(100, classified_pct))\n",
    "        \n",
    "        # Diversity score (0-100)\n",
    "        diversity = tax_results['diversity_index']\n",
    "        scores.append(min(100, diversity * 20))  # Scale diversity to 0-100\n",
    "        \n",
    "        return float(np.mean(scores))\n",
    "\n",
    "# Initialize processor with enhanced capabilities\n",
    "processor = SampleProcessor(config)\n",
    "print(\"Enhanced sample processor initialized with advanced metrics and visualization support.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15a8e2",
   "metadata": {},
   "source": [
    "## Process Initial Subset of Samples\n",
    "\n",
    "We'll now process a small subset of our candidates to:\n",
    "1. Validate our pipeline\n",
    "2. Generate initial QC metrics\n",
    "3. Estimate full processing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d7891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select initial subset for processing with enhanced visualization\n",
    "pilot_size = 5  # Start with 5 samples for initial testing\n",
    "pilot_samples = candidates_df[candidates_df['passes_criteria']].head(pilot_size)\n",
    "\n",
    "# Process pilot samples\n",
    "print(f\"Processing {pilot_size} pilot samples...\")\n",
    "pilot_results = []\n",
    "for _, sample in tqdm(pilot_samples.iterrows(), total=pilot_size):\n",
    "    result = processor.process_sample(sample['accession'])\n",
    "    if result:\n",
    "        pilot_results.append(result)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "pilot_df = pd.DataFrame(pilot_results)\n",
    "\n",
    "# Create comprehensive visualization dashboard\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Quality Score Distribution',\n",
    "        'Taxonomic Classification',\n",
    "        'GC Content Distribution',\n",
    "        'Read Length Distribution',\n",
    "        'Per-Base Quality Heatmap',\n",
    "        'Diversity Index by Sample'\n",
    "    ),\n",
    "    vertical_spacing=0.12\n",
    ")\n",
    "\n",
    "# 1. Quality Score Distribution\n",
    "quality_scores = [r['quality_score'] for r in pilot_results]\n",
    "fig.add_trace(\n",
    "    go.Box(y=quality_scores, name='Quality Scores',\n",
    "           boxpoints='all', jitter=0.3, pointpos=-1.8),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Taxonomic Classification\n",
    "for i, result in enumerate(pilot_results):\n",
    "    top_taxa = pd.DataFrame(result['taxonomic_profile']['top_taxa'])\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=top_taxa['name'], y=top_taxa['percentage'],\n",
    "               name=result['accession']),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# 3. GC Content Distribution\n",
    "for i, result in enumerate(pilot_results):\n",
    "    gc_data = result['qc_metrics']['gc_content']['gc_distribution']\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(gc_data.keys()), y=list(gc_data.values()),\n",
    "                  name=result['accession'], mode='lines'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. Read Length Distribution\n",
    "for i, result in enumerate(pilot_results):\n",
    "    length_data = result['qc_metrics']['sequence_length']['distribution']\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(length_data.keys()), y=list(length_data.values()),\n",
    "                  name=result['accession'], mode='lines'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "# 5. Per-Base Quality Heatmap\n",
    "quality_trends = np.array([r['qc_metrics']['sequence_quality']['quality_trend'] \n",
    "                          for r in pilot_results])\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=quality_trends, \n",
    "               colorscale='Viridis',\n",
    "               showscale=True),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Diversity Index\n",
    "diversity_scores = [r['taxonomic_profile']['diversity_index'] for r in pilot_results]\n",
    "accessions = [r['accession'] for r in pilot_results]\n",
    "fig.add_trace(\n",
    "    go.Bar(x=accessions, y=diversity_scores,\n",
    "           name='Shannon Diversity Index'),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1200,\n",
    "    showlegend=True,\n",
    "    title_text=\"Enhanced Sample Quality Analysis Dashboard\"\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Generate comprehensive statistical summary\n",
    "print(\"\\nPilot Processing Results Summary:\")\n",
    "print(\"================================\")\n",
    "print(f\"Successfully processed: {len(pilot_df)}/{pilot_size}\")\n",
    "\n",
    "print(\"\\nQuality Metrics Summary:\")\n",
    "print(\"----------------------\")\n",
    "quality_summary = pd.DataFrame([{\n",
    "    'Metric': 'Overall Quality Score',\n",
    "    'Mean': np.mean(quality_scores),\n",
    "    'Std': np.std(quality_scores),\n",
    "    'Min': np.min(quality_scores),\n",
    "    'Max': np.max(quality_scores)\n",
    "}])\n",
    "print(quality_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\nTaxonomic Classification Summary:\")\n",
    "print(\"-------------------------------\")\n",
    "for result in pilot_results:\n",
    "    print(f\"\\nSample: {result['accession']}\")\n",
    "    print(f\"Classified Reads: {result['taxonomic_profile']['classified_reads_percent']:.2f}%\")\n",
    "    print(f\"Unique Taxa: {result['taxonomic_profile']['unique_taxa']}\")\n",
    "    print(f\"Diversity Index: {result['taxonomic_profile']['diversity_index']:.2f}\")\n",
    "    print(\"\\nTop 5 Taxa:\")\n",
    "    for taxon in result['taxonomic_profile']['top_taxa'][:5]:\n",
    "        print(f\"  {taxon['name']:50}: {taxon['percentage']:6.2f}%\")\n",
    "\n",
    "# Save enhanced pilot results\n",
    "output_file = RESULTS_DIR / 'enhanced_pilot_analysis.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    serializable_results = []\n",
    "    for result in pilot_results:\n",
    "        serializable = {\n",
    "            'accession': result['accession'],\n",
    "            'quality_score': float(result['quality_score']),\n",
    "            'taxonomic_profile': {\n",
    "                k: (float(v) if isinstance(v, np.float32) else v)\n",
    "                for k, v in result['taxonomic_profile'].items()\n",
    "            },\n",
    "            'qc_metrics': result['qc_metrics']\n",
    "        }\n",
    "        serializable_results.append(serializable)\n",
    "    json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nEnhanced analysis results saved to {output_file}\")\n",
    "print(\"\\nInteractive visualization dashboard generated with comprehensive metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15090091",
   "metadata": {},
   "source": [
    "## Resource Estimation\n",
    "\n",
    "Based on our pilot processing, we'll now:\n",
    "1. Calculate storage requirements\n",
    "2. Estimate compute needs\n",
    "3. Project total processing time\n",
    "4. Generate cost estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc80e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate resource requirements\n",
    "def estimate_resources(pilot_df, full_dataset_size):\n",
    "    \"\"\"Estimate resource requirements based on pilot results.\"\"\"\n",
    "    # Storage estimates\n",
    "    avg_sample_size = candidates_df['estimated_size_gb'].mean()\n",
    "    total_storage_gb = avg_sample_size * full_dataset_size\n",
    "    \n",
    "    # Compute time estimates (based on pilot processing times)\n",
    "    avg_processing_time = 0.5  # hours per sample (placeholder - implement actual timing)\n",
    "    total_processing_time = avg_processing_time * full_dataset_size\n",
    "    \n",
    "    # Cost estimates (placeholder values - adjust based on your cloud provider)\n",
    "    storage_cost_per_gb = 0.02  # $/GB/month\n",
    "    compute_cost_per_hour = 1.0  # $/hour\n",
    "    \n",
    "    estimates = {\n",
    "        'storage': {\n",
    "            'total_gb': total_storage_gb,\n",
    "            'cost_per_month': total_storage_gb * storage_cost_per_gb\n",
    "        },\n",
    "        'compute': {\n",
    "            'total_hours': total_processing_time,\n",
    "            'cost': total_processing_time * compute_cost_per_hour\n",
    "        },\n",
    "        'total_cost_estimate': (total_storage_gb * storage_cost_per_gb + \n",
    "                              total_processing_time * compute_cost_per_hour)\n",
    "    }\n",
    "    \n",
    "    return estimates\n",
    "\n",
    "# Calculate estimates for full PoC (30 samples) and expanded pilot (15 samples)\n",
    "poc_estimates = estimate_resources(pilot_df, 30)\n",
    "pilot_estimates = estimate_resources(pilot_df, 15)\n",
    "\n",
    "# Save estimates\n",
    "estimates = {\n",
    "    'full_poc': poc_estimates,\n",
    "    'expanded_pilot': pilot_estimates,\n",
    "    'calculation_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'estimated_costs.json', 'w') as f:\n",
    "    json.dump(estimates, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "print(\"Resource Estimates:\")\n",
    "print(\"\\nExpanded Pilot (15 samples):\")\n",
    "print(f\"Storage: {pilot_estimates['storage']['total_gb']:.1f} GB\")\n",
    "print(f\"Processing Time: {pilot_estimates['compute']['total_hours']:.1f} hours\")\n",
    "print(f\"Estimated Cost: ${pilot_estimates['total_cost_estimate']:.2f}\")\n",
    "\n",
    "print(\"\\nFull PoC (30 samples):\")\n",
    "print(f\"Storage: {poc_estimates['storage']['total_gb']:.1f} GB\")\n",
    "print(f\"Processing Time: {poc_estimates['compute']['total_hours']:.1f} hours\")\n",
    "print(f\"Estimated Cost: ${poc_estimates['total_cost_estimate']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb427afc",
   "metadata": {},
   "source": [
    "## Generate Final Reports and Recommendations\n",
    "\n",
    "Now we'll:\n",
    "1. Generate the QC report\n",
    "2. Create the recommended expanded pilot list\n",
    "3. Prepare final deliverables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select samples for expanded pilot\n",
    "def select_expanded_pilot(candidates_df, pilot_df, n_samples=12, n_backups=6):\n",
    "    \"\"\"Select samples for expanded pilot based on our criteria.\"\"\"\n",
    "    # Combine metrics from candidates and pilot results\n",
    "    merged_df = candidates_df.merge(pilot_df, on='accession', how='left')\n",
    "    \n",
    "    # Score each sample\n",
    "    merged_df['total_score'] = (\n",
    "        merged_df['metadata_completeness'] * 0.4 +\n",
    "        merged_df['fungal_percent'].fillna(0) * 0.4 +\n",
    "        (merged_df['passes_criteria'].astype(int) * 100) * 0.2\n",
    "    )\n",
    "    \n",
    "    # Sort by score and select top samples\n",
    "    recommended = merged_df.nlargest(n_samples, 'total_score')\n",
    "    backups = merged_df[~merged_df['accession'].isin(recommended['accession'])]\n",
    "    backups = backups.nlargest(n_backups, 'total_score')\n",
    "    \n",
    "    # Prepare recommendation DataFrame\n",
    "    recommendations = pd.concat([\n",
    "        recommended.assign(status='recommended'),\n",
    "        backups.assign(status='backup')\n",
    "    ])\n",
    "    \n",
    "    recommendations['rationale'] = recommendations.apply(\n",
    "        lambda x: f\"Metadata: {x['metadata_completeness']:.1f}%, \"\n",
    "                 f\"Fungal: {x.get('fungal_percent', 'N/A')}%, \"\n",
    "                 f\"Size: {x['estimated_size_gb']:.1f}GB\",\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "expanded_pilot = select_expanded_pilot(candidates_df, pilot_df)\n",
    "expanded_pilot.to_csv(RESULTS_DIR / 'recommended_expanded_pilot.csv', index=False)\n",
    "\n",
    "# Generate HTML report\n",
    "def generate_qc_report():\n",
    "    \"\"\"Generate QC report in HTML format.\"\"\"\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    # Create report\n",
    "    with open(RESULTS_DIR / 'QC_report.html', 'w') as f:\n",
    "        # Write header\n",
    "        f.write(\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>MycoGraph-XL EDA Results</title>\n",
    "            <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "        </head>\n",
    "        <body>\n",
    "        <div class=\"container mt-5\">\n",
    "        <h1>MycoGraph-XL: Exploratory Data Analysis Report</h1>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Add summary statistics\n",
    "        f.write(f\"\"\"\n",
    "        <h2>Summary Statistics</h2>\n",
    "        <ul>\n",
    "            <li>Total candidates: {len(candidates_df)}</li>\n",
    "            <li>Passing all criteria: {candidates_df['passes_criteria'].sum()}</li>\n",
    "            <li>Average metadata completeness: {candidates_df['metadata_completeness'].mean():.1f}%</li>\n",
    "        </ul>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Add visualizations\n",
    "        # (In practice, add plotly figures here)\n",
    "        \n",
    "        f.write(\"</div></body></html>\")\n",
    "\n",
    "# Generate reports\n",
    "generate_qc_report()\n",
    "\n",
    "print(\"\\nFinal Deliverables Generated:\")\n",
    "print(\"1. recommended_expanded_pilot.csv\")\n",
    "print(\"2. QC_report.html\")\n",
    "print(\"3. estimated_costs.json\")\n",
    "print(\"\\nRecommended next steps:\")\n",
    "print(\"1. Review expanded pilot recommendations\")\n",
    "print(\"2. Validate resource estimates\")\n",
    "print(\"3. Proceed with expanded pilot upon approval\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
